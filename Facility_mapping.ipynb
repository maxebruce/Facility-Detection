{"cells":[{"cell_type":"markdown","metadata":{"id":"MxLzb76HD_c-"},"source":["### Setup"]},{"cell_type":"markdown","metadata":{"id":"HpDL3wKx6kh1"},"source":["# Facility Mapping Test 7/10 - Mapping out sub basin in Eagleford"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28241,"status":"ok","timestamp":1752501238478,"user":{"displayName":"Max Bruce","userId":"03723525121176591960"},"user_tz":300},"id":"idj7B3w1E037","outputId":"f582506f-9549-4e0e-e97d-3ac84acb8670"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","import os\n","drive.mount('/content/gdrive') # only if using google colab\n","\n","root = '/content/gdrive/My Drive/your/path' #Path where this notebook is located\n","os.chdir(root)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":254697,"status":"ok","timestamp":1752501498161,"user":{"displayName":"Max Bruce","userId":"03723525121176591960"},"user_tz":300},"id":"LGHBkttJCjKv","outputId":"aca775c8-676b-4f33-9785-febb6c968714"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/facebookresearch/detectron2.git\n","  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-7we6w6i6\n","  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-7we6w6i6\n","  Resolved https://github.com/facebookresearch/detectron2.git to commit 18f69583391e5040043ca4f4bebd2c60f0ebfde0\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (11.2.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.10.0)\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.0.10)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.1.0)\n","Collecting yacs>=0.1.8 (from detectron2==0.6)\n","  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (0.9.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.1.1)\n","Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (4.67.1)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.18.0)\n","Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n","  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n","  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n","Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.3.0)\n","Collecting hydra-core>=1.1 (from detectron2==0.6)\n","  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n","Collecting black (from detectron2==0.6)\n","  Downloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (24.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2.0.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n","Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n","  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (8.2.1)\n","Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n","  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n","Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n","  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (4.3.8)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (4.58.5)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.73.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.8.2)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (5.29.5)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (75.2.0)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.2)\n","Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n","Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Downloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n","Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n","Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n","Building wheels for collected packages: detectron2, fvcore\n","  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for detectron2: filename=detectron2-0.6-cp311-cp311-linux_x86_64.whl size=6434054 sha256=975bd4db2dff50bd03af5be6256b3c8469ae7597b191d0c2e362ea57a6e58cb6\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-74w0jwi9/wheels/17/d9/40/60db98e485aa9455d653e29d1046601ce96fe23647f60c1c5a\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=a65ee33f071ad235878df6f45c90178c4b46990a3e21d39714ce8bc5e0cad1e9\n","  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n","Successfully built detectron2 fvcore\n","Installing collected packages: yacs, portalocker, pathspec, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n","Successfully installed black-25.1.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.1.0 pathspec-0.12.1 portalocker-3.2.0 yacs-0.1.8\n","Requirement already satisfied: folium in /usr/local/lib/python3.11/dist-packages (0.19.7)\n","Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from folium) (0.8.1)\n","Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.11/dist-packages (from folium) (3.1.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from folium) (2.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from folium) (2.32.3)\n","Requirement already satisfied: xyzservices in /usr/local/lib/python3.11/dist-packages (from folium) (2025.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.9->folium) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (2025.7.9)\n","Collecting selenium\n","  Downloading selenium-4.34.2-py3-none-any.whl.metadata (7.5 kB)\n","Collecting urllib3~=2.5.0 (from urllib3[socks]~=2.5.0->selenium)\n","  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n","Collecting trio~=0.30.0 (from selenium)\n","  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n","Collecting trio-websocket~=0.12.2 (from selenium)\n","  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n","Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.7.9)\n","Requirement already satisfied: typing_extensions~=4.14.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.14.1)\n","Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n","Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n","Collecting outcome (from trio~=0.30.0->selenium)\n","  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n","Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n","  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.5.0->selenium) (1.7.1)\n","Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n","Downloading selenium-4.34.2-py3-none-any.whl (9.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m144.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n","Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n","Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n","Installing collected packages: wsproto, urllib3, outcome, trio, trio-websocket, selenium\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 2.4.0\n","    Uninstalling urllib3-2.4.0:\n","      Successfully uninstalled urllib3-2.4.0\n","Successfully installed outcome-1.3.0.post0 selenium-4.34.2 trio-0.30.0 trio-websocket-0.12.2 urllib3-2.5.0 wsproto-1.2.0\n"]}],"source":["!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git' # installs detecron2, be warned this does take a bit (~5-10 minutes)\n","!python -m pip install folium\n","!python -m pip install selenium"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1752503361402,"user":{"displayName":"Max Bruce","userId":"03723525121176591960"},"user_tz":300},"id":"gTlZPJeyCSzH"},"outputs":[],"source":["\n","\n","from detectron2.config import get_cfg\n","from detectron2 import model_zoo\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.utils.visualizer import ColorMode\n","\n","\n","#comment the above out if your not using detectron\n","\n","from google.colab.patches import cv2_imshow\n","\n","#Imports for model evaluation\n","import cv2\n","\n","#imports for SIC\n","\n","import numpy as np\n","import os\n","import tempfile\n","from selenium import webdriver\n","from selenium.webdriver.chrome.options import Options\n","import folium\n","import time\n","from PIL import Image\n","\n","#imports for SIC test\n","import random #need a simulated environment to test coords w/o using AI all the time\n","\n","#Imports for search area creation\n","import matplotlib.pyplot as plt\n","from scipy.stats import gaussian_kde\n","import math\n","from shapely.geometry import Polygon, box\n","\n","#imports for file operations\n","from datetime import datetime"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":43,"status":"ok","timestamp":1752503359238,"user":{"displayName":"Max Bruce","userId":"03723525121176591960"},"user_tz":300},"id":"AajobbC7Dzee"},"outputs":[],"source":["\n","import json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"executionInfo":{"elapsed":1102,"status":"error","timestamp":1752164554813,"user":{"displayName":"Max Bruce","userId":"03723525121176591960"},"user_tz":300},"id":"ac1j-Em5HwDj","outputId":"c64f3142-1a60-4c05-bf2e-aeb572d2b506"},"outputs":[{"ename":"AssertionError","evalue":"Checkpoint ./output/model_final.pth not found!","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-14-2573727909.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mROI_HEADS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSCORE_THRESH_TEST\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m   \u001b[0;31m# threshold where the program returns a positive hit. Below this the program will return nothing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# The threshold described in the parameters is the score needed to go to the reivew file (ie. useful for our purposes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDefaultPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mcheckpointer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDetectionCheckpointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mcheckpointer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWEIGHTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         self.aug = T.ResizeShortestEdge(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/detectron2/checkpoint/detection_checkpoint.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeturl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# remove query from filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fvcore/common/checkpoint.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, checkpointables)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Checkpoint {} not found!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: Checkpoint ./output/model_final.pth not found!"]}],"source":["#Functions for setup\n","from detectron2.engine import DefaultPredictor\n","cfg = get_cfg()\n","cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n","cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model being used (this may change in the future)\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 5 #Enter the amount of classes you are using (we currently have 4 for production mapping)\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # threshold where the program returns a positive hit. Below this the program will return nothing.\n","# The threshold described in the parameters is the score needed to go to the reivew file (ie. useful for our purposes)\n","predictor = DefaultPredictor(cfg)"]},{"cell_type":"markdown","metadata":{"id":"OxLdqnTgEFrs"},"source":["### SIC Functions"]},{"cell_type":"markdown","metadata":{"id":"SE0KsJxmhpv4"},"source":["Below are the functions which scale SIC up to basin-scale processing"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":40,"status":"ok","timestamp":1752501506545,"user":{"displayName":"Max Bruce","userId":"03723525121176591960"},"user_tz":300},"id":"tio3XHD5DdK9"},"outputs":[],"source":["'''\n","kde_finding takes a list of coordinates (same data type as SIC), and the diagnols of each 2 mile box. It then generates a heatmap of the most likley location for the next site\n","the returns are the list of the top five boxes most likley to contain another site.\n","\n","note - this is based on a gaussian distirbution. Sites may rely on a varigram-derived distribution or logistical-based features in practice\n","\n","in the future, this function must be developed to accomodate kriging or logistical elements\n","- - EXAMPLE:\n","- - for instance, if we know a central processing facility is somwhere, the chances of wells/gathering facilities nearby are high\n","- - likewise, compressor stations are only clustered on the sub-basin scale. So KDE will not be useful. In fact, the opposite is true (incorporate this in next steps)\n","- - Wells and wellpads however will be clustered on the sub-basin scale, so KDE will be useful\n","\n","'''\n","\n","\n","def kde_finding(coords, diagonals):\n","\n","    #make sure your coords are within the diagnol polygon\n","    #prob wont cause errors, but will mess up the program\n","\n","    # === 1. Input coordinates (x, y) ===\n","    coordinates = np.array(coords)\n","\n","    # (lattitude, longitude)\n","    # (    Y    ,    X     )\n","\n","    # === 2. Extract x and y arrays ===\n","    x = coordinates[:, 1]\n","    y = coordinates[:, 0]\n","\n","    # Y is lattitude\n","    # X is longitude\n","    # The above snippet solves this\n","\n","\n","\n","    # === 3. Apply KDE ===\n","    xy = np.vstack([x, y])\n","    kde = gaussian_kde(xy)\n","\n","    # === 4. Define KDE grid ===\n","    xmin, xmax = x.min() - 0.1, x.max() + 0.1\n","    ymin, ymax = y.min() - 0.1, y.max() + 0.1\n","    xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n","    grid_coords = np.vstack([xx.ravel(), yy.ravel()])\n","    zz = kde(grid_coords).reshape(xx.shape)\n","\n","    # === 5. Plot heatmap ===\n","    plt.figure(figsize=(8, 6))\n","    plt.imshow(np.rot90(zz), extent=[xmin, xmax, ymin, ymax], cmap='hot', aspect='auto')\n","    plt.scatter(x, y, c='blue', s=10, label='Well Pad Locations')\n","    plt.colorbar(label='Density Estimate')\n","    plt.title('KDE Heatmap of Site Locations')\n","    plt.xlabel('X')\n","    plt.ylabel('Y')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Dont need to divide into boxes as the boxes are already there\n","\n","    hotspot_info = []\n","\n","    for i in diagonals:\n","        x_start = i[0][1]\n","        x_end = i[1][1]\n","        y_start = i[0][0]\n","        y_end = i[1][0]\n","\n","            # Find KDE values within this grid cell\n","        mask = (\n","            (xx >= x_start) & (xx < x_end) &\n","            (yy <= y_start) & (yy > y_end)\n","        )\n","        cell_density = zz[mask].mean() if np.any(mask) else 0\n","\n","        hotspot_info.append({\n","            'density': cell_density,\n","            'bounding_box': ((y_start, x_start), (y_end, x_end))\n","        })\n","\n","    # === 7. Sort and return top 10 cells with highest density ===\n","    top_cells = sorted(hotspot_info, key=lambda x: x['density'], reverse=True)[:5]\n","    diag_list = []\n","\n","    # === 8. Print results ===\n","    print(\"\\nTop 10 grid cells most likely to contain another site:\")\n","    for i, cell in enumerate(top_cells, 1):\n","        bbox = cell['bounding_box']\n","        diag_list.append((cell['bounding_box'], cell['density']))\n","        print(f\"{i:2d}. Density: {cell['density']:.4f}, Bounding Box: {bbox}\")\n","\n","    #now, we have the diagnols of the top 10 boxes most likley to have another coord\n","    #These coords can also be tagged to make sure theres no double dipping\n","    #Reminder: the return is a tuple\n","\n","    return(diag_list)\n","\n","\n","def little_boxes(poly_coords):\n","    '''\n","    Changed the test code, now the longs per mile is dependent on the first polygon coord\n","    Should make this more adaptaple, but doesnt matter unless youre in the arctic or sumn\n","    '''\n","    LAT_PER_MILE = 1 / 69  # degrees per mile for latitude\n","    LON_PER_MILE = 1 / (69 * math.cos(math.radians(poly_coords[0][0])))  # degrees per mile for longitude\n","    CELL_SIZE_MILES = 2\n","    DELTA_LAT = CELL_SIZE_MILES * LAT_PER_MILE\n","    DELTA_LON = CELL_SIZE_MILES * LON_PER_MILE\n","\n","    #Code still works with this change\n","\n","    # === 2. Input: Your polygon (replace with your coordinates) ===\n","    # Format: list of (lon, lat)\n","    polygon = Polygon(poly_coords)\n","\n","    # === 3. Get bounding box of polygon ===\n","    min_lon, min_lat, max_lon, max_lat = polygon.bounds\n","\n","    # === 4. Generate 2-mile boxes ===\n","    grid_boxes = []\n","\n","    lat = min_lat\n","    while lat < max_lat:\n","        lon = min_lon\n","        while lon < max_lon:\n","            cell = box(lon, lat, lon + DELTA_LON, lat + DELTA_LAT)  # Create box\n","            if polygon.contains(cell):  # Keep if it overlaps the polygon\n","                grid_boxes.append(cell)\n","            lon += DELTA_LON\n","        lat += DELTA_LAT\n","\n","\n","    # === 5. Output: Print coordinates of boxes ===\n","    print(f\"Generated {len(grid_boxes)} 2-mile x 2-mile boxes intersecting the polygon.\\n\")\n","    diagonals = []\n","\n","    for i, cell in enumerate(grid_boxes):\n","        maxy, minx, miny, maxx = cell.bounds\n","        diagonal = ((miny, minx), (maxy, maxx))  # From upper left to lower right\n","        diagonals.append(diagonal)\n","    # === 6. Optional: Plot the grid and the polygon ===\n","    fig, ax = plt.subplots(figsize=(8, 6))\n","    x, y = polygon.exterior.xy\n","    ax.plot(y, x, color='blue', label='Polygon')\n","\n","    for cell in grid_boxes:\n","        x, y = cell.exterior.xy\n","        ax.plot(y, x, color='red', linewidth=0.5)\n","\n","    ax.set_title(\"2-Mile x 2-Mile Grid Cells in Polygon\")\n","    ax.set_xlabel(\"Longitude\")\n","    ax.set_ylabel(\"Latitude\")\n","    ax.legend()\n","    plt.grid(True)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return diagonals\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fCt54aiEmfGP"},"source":["### Below are the original SIC functions"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1752512106887,"user":{"displayName":"Max Bruce","userId":"03723525121176591960"},"user_tz":300},"id":"i6BYh8EtDpQ6"},"outputs":[],"source":["def generate_coords(diag_pt1, diag_pt2, num_points):\n","    \"\"\"\n","    Generate a list of equally spaced coordinates within a square\n","    defined by two diagonal points.\n","\n","    Parameters:\n","        diag_pt1 (tuple): (x1, y1) - one corner of the square\n","        diag_pt2 (tuple): (x2, y2) - opposite corner of the square\n","        num_points (int): number of coordinates to generate\n","\n","    Returns:\n","        list of (x, y) tuples\n","    \"\"\"\n","\n","    # Get the bounds\n","    x_min, x_max = sorted([diag_pt1[0], diag_pt2[0]])\n","    y_min, y_max = sorted([diag_pt1[1], diag_pt2[1]])\n","\n","    # Calculate number of points per side to form a grid\n","    side_points = int(np.ceil(np.sqrt(num_points)))\n","\n","    # Generate a grid of x and y values\n","    x_vals = np.linspace(x_min, x_max, side_points)\n","    y_vals = np.linspace(y_min, y_max, side_points)\n","\n","    print(f\"X vals {x_vals}\")\n","    print(f\"Y VALS {y_vals}\")\n","\n","    coords = [(x, y) for x in x_vals for y in y_vals]\n","\n","    print(coords)\n","\n","    # Trim to exactly `num_points`\n","    return coords #we want all the points in the grid, cant trim off the top few\n","\n","\n","def launch_driver():\n","    '''\n","    When writing this, there was a reucurring issue with the driver, this function fixed it, so we need to call it only once, idk why\n","\n","    returns:\n","    driver - object\n","\n","    '''\n","    chrome_options = Options()\n","    chrome_options.add_argument(\"--headless\")\n","    chrome_options.add_argument(\"--no-sandbox\")\n","    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n","    chrome_options.add_argument(\"--window-size=800,800\")\n","    chrome_options.add_argument(f\"--user-data-dir={tempfile.mkdtemp()}\")\n","    chrome_options.add_argument(f\"--remote-debugging-port=9222\")\n","    return webdriver.Chrome(options=chrome_options)\n","\n","\n","def master_coords(coordinates, directory):\n","    '''\n","    Takes a list of coordinates and returns images of them\n","\n","    Parameters:\n","\n","    list of tuples of coordinates [(lat, long), (lat, long), ....]\n","    Latitude and longitude are in decimal system with positives and negatives, not degress/minutes/seconds system\n","\n","    directory - file where you want your images sent to\n","\n","    NOTE: This function is a major time bottlneck for SIC, because it uses a web driver which physically navigates Esri's imagery database\n","    Once I integrate sentinel (soon), this could cut down SIC's operation time signficantly\n","\n","    Similarly, because this uses a webdriver, artifacts from the webpage may exist on the output images (watermarks, zoom buttons)\n","    '''\n","\n","\n","\n","    zoom = 18 #how much were zooming in\n","    output_dir = os.path.join(root, directory) # directory were saving images to\n","    os.makedirs(output_dir, exist_ok=True) #actually making this directory\n","\n","    driver = launch_driver()  # Launch once\n","\n","    # === Generate map HTML with Esri imagery ===\n","    def generate_satellite_map(lat, lon, zoom, html_filename):\n","        m = folium.Map(location=[lat, lon], zoom_start=zoom, tiles=None) #folium map\n","\n","        # Add Esri World Imagery\n","        folium.TileLayer(\n","            tiles=\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\",\n","            attr=\"Esri\",\n","            name=\"Esri Satellite\",\n","            overlay=False,\n","            control=False,\n","        ).add_to(m)\n","\n","        m.save(html_filename)\n","\n","    '''\n","    The previous function creates an html file with all the map stuff. This is not usable for the AI. It must be converted to JPEG\n","    '''\n","\n","\n","    def html_to_jpeg(html_file, jpeg_output_file):\n","        driver.get(\"file://\" + os.path.abspath(html_file))\n","        time.sleep(2)  # wait for map tiles to load\n","\n","        temp_png = jpeg_output_file.replace(\".jpg\", \"_temp.png\")\n","        driver.save_screenshot(temp_png)\n","\n","        with Image.open(temp_png) as im:\n","            rgb_im = im.convert(\"RGB\")\n","            rgb_im.save(jpeg_output_file, \"JPEG\", quality=95)\n","\n","        os.remove(temp_png)\n","\n","    '''\n","    This whole process takes a long time, and each images takes a few seconds to generate, this means only small areas should be analyzed to stay in computation limits\n","    '''\n","\n","    '''\n","    Below, we loop through each coordinate, generating an image, we also delete the html file\n","    '''\n","    # === Run for each coordinate ===\n","    for idx, (lat, lon) in enumerate(coordinates):\n","        image_error = False\n","        html_file = f\"temp_map_{idx}.html\"\n","        jpeg_file = os.path.join(output_dir, f\"satellite_{idx+1}_{lat}_{lon}.jpg\")\n","\n","        try:\n","          generate_satellite_map(lat, lon, zoom, html_file)\n","          html_to_jpeg(html_file, jpeg_file)\n","\n","          print(f\"Saved JPEG: {jpeg_file}\")\n","          os.remove(html_file)  # optional: clean up\n","\n","        except:\n","          break_line = input(f\"Image Aquisition Error or kernal interrupted, to proceed type anything. To end type END, and then stop the kernal\")\n","          image_error = True\n","          if break_line == \"END\": #Program will proceed\n","            driver.quit() #Quits the driver to prevent issues\n","            end_input = input(\"You may now end the program by stopping the kernal\")\n","\n","    driver.quit()\n","\n","'''\n","This is the core of SIC\n","'''\n","#upper left coordinate, lower right coordinate, # of images taken, directory to evaluate files, boolean - set TRUE if you want to keep all images for training, boolean - set to TRUE if you want to input your own coordinates,  number of files in batch\n","def foureightysix(diagUL, diagLR, resolution , directory, threshhold, datamining, preset, tolerance, sampling, sample_size, model_name, looped):\n","\n","  #set preset to True to input your own set of coordinates, make sure batch is equal to number of coordinates in this case\n","  #tolerance is the confidence in which the model thinks a site is a compressor station, lower tolerance will increase the false positives, but also decrease false negatives\n","  #sampling - boolean - are we taking a sample of the coords instead of taking images of every single one\n","  #sample size - percetnage of array we are actually taking coords of - important to keep within computing constraints\n","\n","  print(f\"#########################STARTING NEW COORDINATE SET \\n ##########################{diagUL}, {diagLR}######################################\")\n","\n","  if preset == False:\n","    coordinates = generate_coords(diagUL, diagLR, resolution)\n","\n","    coordinates = [(float(lat), float(lon)) for lat, lon in coordinates]\n","\n","  if preset == True:\n","    coordinates = diagUL\n","\n","  print(f\"All coordinates: {coordinates}\")\n","\n","  if sampling == True:\n","    coordinates = random.sample(coordinates, sample_size)\n","\n","  print(\"Coordinates generated\")\n","\n","  '''\n","  This part splits the coordinate list into batches. This allows the program to delete images we dont need in the next step\n","\n","  '''\n","\n","  def chunks(lst, n):\n","    \"\"\"Return a list of n-sized chunks from lst.\"\"\"\n","    return [lst[i:i + n] for i in range(0, len(lst), n)]\n","\n","  coordinates_chunked = chunks(coordinates, threshhold)\n","  cord_batch = 1\n","\n","  # displays each batch and their coordinates\n","\n","  for i in coordinates_chunked:\n","    print(f\"Batch {cord_batch}: {i}\")\n","    cord_batch += 1\n","\n","  print(\"Chunks divided\")\n","\n","  '''\n","  Importing modules again just to be sure. The threshold (no relation to the parameter above) used is 0.5\n","  '''\n","\n","  print(\"Importing model\")\n","\n","  from detectron2.engine import DefaultPredictor\n","  cfg = get_cfg()\n","  cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n","  cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, model_name)  # path to the model being used (this may change in the future)\n","  cfg.MODEL.ROI_HEADS.NUM_CLASSES = 5\n","  cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set a custom testing threshold\n","  predictor = DefaultPredictor(cfg)\n","\n","\n","  print(\"Model successfully called\")\n","\n","  #Each file is called in the below dictionary, and assined a value depending on its need for user review\n","  #Results is the sets of coordinates we are returning\n","\n","  master_dictionary = {}\n","  results = []\n","\n","  i = 0\n","\n","\n","  print(coordinates_chunked)\n","\n","  files_for_review = os.path.join(root, f\"{directory} REVIEW FOLDER\")\n","  os.makedirs(files_for_review, exist_ok=True)\n","\n","  for batch in coordinates_chunked:\n","\n","    print(\"##############   BEGINING NEW BATCH    #################\")\n","\n","    master_coords(batch, directory) # This generates the batch of images\n","\n","    print(\"- - - IMAGES FOR THIS BATCH SUCCESSFULLY GENERATED\")\n","\n","    '''\n","    Looks for the directory created earlier\n","    '''\n","\n","    files_path_eval = os.path.join(root, directory) # the directory were using\n","    time.sleep(1) #Need some time for the folder to generate, otherwise errors\n","    files_eval = os.listdir(files_path_eval) #makes a list of files in the directory, each one will be called\n","\n","    print(\"- - Directory created/called\")\n","\n","    '''\n","    Here, the AI is brought in, we loop through the specified directory and evaluate each image\n","\n","    '''\n","    print(\"- - Looping through files\")\n","\n","    for file in files_eval:\n","      print(f\"\\n\\n #######################Analyzing {file}################################\")\n","      site_dict = {}\n","      path = os.path.join(files_path_eval, file)\n","      im = cv2.imread(path) #puts the file into the image interface\n","      outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n","      v = Visualizer(im[:, :, ::-1],\n","                      scale=3.0,\n","                      instance_mode=ColorMode.IMAGE_BW\n","      )\n","\n","      save_path = os.path.join(files_for_review, file)\n","\n","      '''\n","      try:\n","\n","        # If the detection is signficant, but not very high\n","        if outputs[\"instances\"].scores.tolist()[0] < tolerance:\n","          cv2.imwrite(save_path, im) #saves to review stack\n","          print(f\"{file} Has a low Confidence interval (0.5 < C.I. < tolerance), investigate (training error? Wrong building?)\")\n","          print(outputs[\"instances\"].scores.tolist()[0])\n","          out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","          cv2_imshow(out.get_image()[:, :, ::-1])\n","          master_dictionary.update({file: \"LOW\"}) #adds to dict\n","\n","        elif outputs[\"instances\"].scores.tolist()[0] >= tolerance:\n","          cv2.imwrite(save_path, im) # saves to review stack\n","          print(f\"{file} DESIRED BUILDING DETECTED, find in review folder \")\n","          print(outputs[\"instances\"].scores.tolist()[0])\n","          out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","          cv2_imshow(out.get_image()[:, :, ::-1])\n","          master_dictionary.update({file: \"HIGH\"}) # adds to dict\n","\n","      except:\n","          print(f\"{file}: Desired Object not detected, check coordinates on google earth if you think this is a mistake\")\n","          master_dictionary.update({file: \"NONE\"}) #Adds to dict, NONE marks it for removal\n","\n","      print(f\"Completed File {file}\") #Shows the file has been completed, useful for debugging\n","\n","      '''\n","\n","      # Get predictions\n","      instances = outputs[\"instances\"]\n","      pred_classes = instances.pred_classes.cpu().tolist()  # class indices\n","      scores = instances.scores.cpu().tolist()              # confidence scores\n","\n","      #Only do analysis if theres stuff in the image\n","      if len(instances) > 0:\n","        cv2.imwrite(save_path, im) #File goes to review file\n","\n","        #Visualize the image\n","        out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","        cv2_imshow(out.get_image()[:, :, ::-1])\n","\n","        # Map class indices to labels\n","        class_names = [\"tank battery\", \"well pad\", \"artificial lift\", \"separator\", \"compressor\"]\n","        labels = [class_names[i] for i in pred_classes]\n","\n","        # Count each object type\n","        from collections import Counter\n","        counts = Counter(labels)\n","\n","        # Print scores and counts\n","        for label, score in zip(labels, scores):\n","            print(f\"Detected: {label} with confidence {score:.2f}\")\n","            if label not in site_dict:\n","              site_dict[label] = [score]\n","            else:\n","              site_dict[label].append(score)\n","\n","        #filename contains coord data, we extract this to obtain emissions amount, as well as pass to the main dictionary for the output\n","        file_coord = file\n","        file_coord = file_coord.strip(\".jgp\")\n","        file_coord = file_coord.split(\"_\")\n","\n","        '''\n","        Site_sort (see code below) allows us to not have to review every single image. Some combinates of scores and features\n","        are almost always indicative of a actual O&G facility, and are automatically marked as positives\n","\n","        On the other hand, some sites (low confidence, singular pads) are almost never a useful site, and are thrown out of the data\n","\n","        This sorting is very important, and with more data, can be expanded to make manual review only necessary for extreme cases\n","\n","        Some ideas could include:\n","        - - - Looking at the overlap of detection masks\n","        - - -\n","\n","        '''\n","\n","        sort_tag = site_sort(site_dict)\n","\n","        if sort_tag == None:\n","          site_dict['status'] = \"Check\"\n","          print(f\"No Facility could be determined\\n - We record {site_dict}\")\n","          master_dictionary[((float(file_coord[2]), float(file_coord[3])))] = site_dict\n","\n","\n","\n","        elif sort_tag == \"BAD\":\n","          print(\"Only Low Confidence Pad Dectected, likley not a site\")\n","          site_dict['status'] = 'bad'\n","          print(f\"\\nFull Site Dict \\n{site_dict}\\n\")\n","          os.remove(save_path)\n","          #Removes bad data\n","\n","\n","        else:\n","          site_dict['status'] = 'good'\n","          site_dict['Facility Type'] = sort_tag\n","          print(f\"{site_dict['Facility Type']} Detected\")\n","          print(f\"\\nFull Site Dict \\n{site_dict}\\n\")\n","          master_dictionary[((float(file_coord[2]), float(file_coord[3])))] = site_dict\n","\n","      else:\n","        print(f\"{file}: No object not detected, check coordinates on google earth if you think this is a mistake\")\n","        #If theres no hits, dict is not updated and theres no analysis. Should save come computing\n","\n","      print(f\"File {file} analyzed \\n\\n\\n\")\n","\n","    '''\n","    The batch has been properly labelled now, now the folder has to be cleaned out by removing NONE labeled files\n","    '''\n","\n","    print(f\"ALL IMAGES IN BATCH {batch} IMAGES SORTED SUCCESSFULLY, NOW CLEANING FOLDER\")\n","\n","    '''\n","    Below code cleans the folder of files that are not flagged for review. If dataming is true, these files are kept and saved to the drive.\n","    NOTE: turn datamining on only if you have the necessary drive space (extra 2-3 gb)\n","\n","    '''\n","\n","\n","    files_clean_path = os.path.join(root, directory)\n","    files_clean = os.listdir(files_clean_path)\n","    if datamining == False:\n","      for file in files_clean:\n","        path = os.path.join(files_clean_path, file)\n","        os.remove(path)\n","\n","    print(\"FOLDER SUCCESSFULLY CLEANED\")\n","\n","  '''\n","    After batches are done, go through the dictionary and build the results list\n","  '''\n","\n","  #The user must look at each file to determine if its good. The model must always be training! Maybe if the model starts behaving we can leave this alone!\n","  if looped == True: #We dont want bad data being used in the next iteration, so it must be checked\n","    manual_review_486(f\"{directory} REVIEW FOLDER\", master_dictionary)\n","\n","    #Goes through and removes images in the master dict that arent in the review file\n","    post_rev_dict = {}\n","    for i in coords_all:\n","      for file in os.listdir(files_for_review):\n","        file_coord = file\n","        file_coord = file_coord.strip(\".jgp\")\n","        file_coord = file_coord.split(\"_\")\n","        if i == ((float(file_coord[2]), float(file_coord[3]))):\n","          post_rev_dict[i] = coords_all[i]\n","\n","      coords_all = post_rev_dict\n","\n","  else:\n","    print(\"Per-Batch Manual Review Disabled (Manual Review will be done after process)\")\n","\n","\n","\n","\n","  #Replace the master dictionary with the review dictionary\n","\n","  print(f\"\\n\\n\\n\\n\\ #############Results \\n {master_dictionary}\")\n","\n","\n","  return master_dictionary"]},{"cell_type":"code","source":[],"metadata":{"id":"bvPg0DCVnH9C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Dt0adoiH_dC"},"source":["#### 486 Variants\n","We will need some variants of this code for detection purposes\n","1. A variant that analyzes the objects and scores to make a decision on the sites identity"]},{"cell_type":"markdown","metadata":{"id":"ByVM8SJ7H-fd"},"source":[]},{"cell_type":"markdown","metadata":{"id":"jhRfa21dFYH5"},"source":["### Coordmaster Test Runs\n","Puts 486, KDE function, and manual review together to create a list of files with fites, as well as the dictionary with site features"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"9j6VAcfsEI3a","executionInfo":{"status":"ok","timestamp":1752508234422,"user_tz":300,"elapsed":6,"user":{"displayName":"Max Bruce","userId":"03723525121176591960"}}},"outputs":[],"source":["def coord_master_test(poly_coords, looped, run_name, model_name):\n","    #space for coordinates\n","    coords_all = {}\n","    diagonals = []\n","    counter = 0\n","\n","    #Run 486 for each box\n","    boxes = little_boxes(poly_coords)\n","    length = len(boxes)\n","\n","    save_folder_path = os.path.join(root, f\"{run_name} Save File\")\n","    os.makedirs(save_folder_path, exist_ok=True)\n","    for i in boxes:\n","        '''\n","        Counter to check progress through array\n","        '''\n","        counter += 1\n","        print(f\"##########################################################################################\\n\\n\\n BOX {counter}, out of {length}\\n\\n\\n\")\n","        print(f\"##########################################################################################\\n\\n\")\n","        coords_all.update(foureightysix(i[0], i[1], 200, f\"{run_name}\", 200, False, False, 0.50, False, 0.5, model_name, looped)) #Be sure to alter these parameters here\n","        diagonals.append(i)\n","\n","        '''\n","        After each box save the dictionary to a json file, if the program fails the user can access the files\n","        '''\n","\n","        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","        save_name = f\"{run_name}_save{counter}.json\"\n","        save_file_path = os.path.join(save_folder_path, save_name)\n","\n","        # Write dictionary to JSON file\n","        with open(save_file_path, \"w\") as json_file:\n","            json.dump(str(coords_all), json_file, indent=4)\n","\n","        print(f\"Dictionary saved to {filename}\")\n","\n","    coords_all_safe = coords_all.copy() #save a copy of coords just to be safe\n","    print(f\"\\n\\n\\n MASTER DICTIONARY {coords_all_safe}\\n\\n\\n\") #so can be copied in the event of error\n","\n","    #Run manual review\n","    coords_all = manual_review_486(f\"{run_name} REVIEW FOLDER\", coords_all_safe)\n","\n","    post_rev_dict = {}\n","    for i in coords_all:\n","      for file in os.listdir(os.path.join(root, f\"{run_name} REVIEW FOLDER\")):\n","        file_coord = file\n","        file_coord = file_coord.strip(\".jgp\")\n","        file_coord = file_coord.split(\"_\")\n","        if i == ((float(file_coord[2]), float(file_coord[3]))):\n","          post_rev_dict[i] = coords_all[i]\n","\n","    coords_all = post_rev_dict\n","\n","    #cleaning out Nonetype arguments\n","    coords_cleaned = []\n","    for k in coords_all:\n","        if k is not None:\n","            coords_cleaned.append(k)\n","\n","    '''\n","    Plotting\n","    '''\n","\n","    xx = []\n","    yy = []\n","\n","    print(f\"\\n\\n\\n\\n\\ COORDS CLEANED: {coords_cleaned}\")\n","\n","    for coord in coords_cleaned:\n","        print(coord)\n","        xx.append(coord[1])\n","        yy.append(coord[0])\n","\n","    plt.figure(figsize=(6, 6))\n","    plt.scatter(xx, yy, c='red', marker='o')\n","    plt.title(\"Facility Coordinates\")\n","    plt.xlabel(\"Longitude\")\n","    plt.ylabel(\"Latitude\")\n","    plt.grid(True)\n","    plt.axis('equal')  # Keeps aspect ratio consistent\n","    plt.show()\n","\n","    #If we want to put these results back into coord master, we can return KDE results for another round of searching (the next mastercoord can have the sampling turned off)\n","    #otherwise, especially if the results werent sampled, we can just get a list of coords and the diagnols for future use\n","    #I wouldnt reccomend making this process fully automatic, as the models arent entirely accurate (only slightly more accurate than not)\n","    '''\n","    Workflow > Get polygon > run through coordmaster >\n","\n","    '''\n","\n","    # Get current date and time formatted for filename\n","    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","    filename = f\"{run_name}_{timestamp}.json\"\n","\n","    # Write dictionary to JSON file\n","    with open(filename, \"w\") as json_file:\n","        json.dump(str(coords_all), json_file, indent=4)\n","\n","    print(f\"Dictionary saved to {filename}\")\n","\n","    if looped == True:\n","      return kde_finding(coords_cleaned, diagonals)\n","    else:\n","      return (coords_cleaned, diagonals)"]},{"cell_type":"markdown","metadata":{"id":"DMCAwQvWJi1I"},"source":["Manual Review Function - sorts mined data iinto positives and negatives"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1752505748081,"user":{"displayName":"Max Bruce","userId":"03723525121176591960"},"user_tz":300},"id":"MJ-usYWVdfbz"},"outputs":[],"source":["'''\n","This will speed things up, esentially we no longer have to cut and past all those files. Rather we can sort\n","\n","'''\n","\n","def manual_review_486(run_name, master_dictionary):\n","  from google.colab.patches import cv2_imshow #just so we dont have to run everything else\n","  import time\n","  import shutil\n","\n","  reviewed_dict = {}\n","\n","  files_check_path = os.path.join(root, f\"{run_name}\")\n","  files_check = os.listdir(files_check_path)\n","\n","  print(f\"Accessing files in {files_check_path}\")\n","  print(f\"Checking files: {files_check}\")\n","\n","  for file in files_check:\n","    path = os.path.join(files_check_path, file)\n","    im = cv2.imread(path)\n","\n","    print(f\"\\n - - Analyzing File at {path}\")\n","\n","    file_coord = file\n","    file_coord = file_coord.strip(\".jgp\")\n","    file_coord = file_coord.split(\"_\")\n","\n","    user_facility = None\n","    print(f\"\\n\\n ###################################New IMAGE##############################################\\n ########{file}########\")\n","\n","    # a little automation for the sites that are definitley confimed\n","\n","    response = 'no'\n","\n","    try:\n","\n","      if master_dictionary[((float(file_coord[2]), float(file_coord[3])))]['status'] == 'good':\n","        print(\"High facility confidence, moving to positives folder \")\n","        response = 'yes'\n","\n","      else:\n","        print(\"\\n\\n REVIEW REQUIRED\")\n","        cv2_imshow(im)\n","        time.sleep(2)\n","        response = input(\"Type 'yes' to mark as positive, or press Enter to mark as negative: \").strip().lower()\n","        if response == 'yes': #only if the user puts in yes during review, pre-labelled facilities do not need user input\n","          user_facility = input(\"Enter the facility name: \") #User an input the facility name\n","    except:\n","      print(\"Image index error, sending to Negatives\")\n","\n","\n","\n","    # Determine destination\n","    positive_dir = os.path.join(root, f\"{run_name} positives\")\n","    negative_dir = os.path.join(root, f\"{run_name} negatives\")\n","\n","    # Create folders if they don't exist\n","    os.makedirs(positive_dir, exist_ok=True)\n","    os.makedirs(negative_dir, exist_ok=True)\n","\n","\n","    #Send every image, regardless of score, to the training folder\n","    train_path = os.path.join(root, \"data\", \"train\")\n","    os.makedirs(train_path, exist_ok=True)\n","    save = os.path.join(train_path, file)\n","    cv2.imwrite(save, im)\n","\n","    # Move the file to respective folder\n","    filename = os.path.basename(path)\n","    if response == \"yes\":\n","        dest_path = os.path.join(positive_dir, filename)\n","        reviewed_dict[((float(file_coord[2]), float(file_coord[3])))] = master_dictionary[((float(file_coord[2]), float(file_coord[3])))]\n","        if user_facility is not None:\n","          reviewed_dict[((float(file_coord[2]), float(file_coord[3])))]['Facility Type'] = user_facility\n","\n","    else:\n","        dest_path = os.path.join(negative_dir, filename)\n","\n","    shutil.copy(path, dest_path)\n","    print(f\"Image moved to: {dest_path}\")\n","    #Now, dowload each of the folders on google drive, and use the upload tool to send to to training\n","\n","\n","  print(f\"Returning dictionary: {reviewed_dict}\")\n","\n","  return reviewed_dict\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DLD7SZsFJ2RS"},"source":["## Basic Deployment\n","Lets so how this model works in a small test deployment"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1752501507015,"user":{"displayName":"Max Bruce","userId":"03723525121176591960"},"user_tz":300},"id":"KUWa7ehQKA_C","outputId":"decbc329-1dba-475e-8c4b-fd8369bc17ad"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\npoly_4 =\\npoly_5 =\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}],"source":["# Lets start a polygon database\n","poly_1 = [(28.681, -98.226), (28.675, -98.178), (28.584, -98.182), (28.589, -98.204)]\n","\n","poly_2 = [(28.800, -98.098), (28.800, -98.000), (28.771, -98.000), (28.771, -98.098)]\n","\n","poly_3 = [(28.800, -98.068), (28.800, -98.030), (28.400, -98.000), (28.400, -98.098)]\n","\n","'''\n","poly_4 =\n","poly_5 =\n","'''"]},{"cell_type":"markdown","metadata":{"id":"Jth4CYfEP8Hx"},"source":["### Sorting algorithm\n","Currently, im seeing some patterns with the model results. It looks like definite sites have a list of features and scores which can be interpreted\n","\n","A good example are well pads. They have a pad score which is 90 plus. Combine that with any >50 tank battery and it will likley be a well pad barring some cases\n","\n","Compressors will have a pad with 2+ compressors\n","\n","Guarantee a well pad with >75 pad with any artificial lift\n","\n","Processing center if theres 2+ compressors and 1+ separator\n","\n","Upstream facility (well, gathering) (wont be able to determine specific type until we get well recognition) if theres a TB OR Separator + pad."]},{"cell_type":"code","execution_count":57,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1752512179600,"user":{"displayName":"Max Bruce","userId":"03723525121176591960"},"user_tz":300},"id":"eFsiIjfxMskZ"},"outputs":[],"source":["#class_names = [\"tank battery\", \"well pad\", \"artificial lift\", \"separator\", \"compressor\"]\n","\n","def site_sort(site_dict):\n","\n","    tag = []\n","    tank_val = site_dict.get('tank battery', 0)\n","    sep_val = site_dict.get('separator', 0)\n","    lift_val = site_dict.get('artificial lift', 0)\n","    comp_val = site_dict.get('compressor', 0)\n","    pad_val = site_dict.get('well pad', 0)\n","\n","    #Case 0: Processing Facility\n","\n","    if 'separator' in site_dict and 'compressor' in site_dict:\n","        if any(x > 0.5 for x in site_dict['separator']) and len(site_dict['compressor']) > 2:\n","            tag.append(\"Processing facility \")\n","\n","    # Case 1: Compressor Station\n","    elif 'well pad' in site_dict and 'compressor' in site_dict:\n","        if any(x > 0.5 for x in site_dict['well pad']) and len(site_dict['compressor']) > 1:\n","            tag.append(\"Compressor Station\")\n","\n","    # Case 2: Upstream Facility\n","    elif 'tank battery' in site_dict or 'separator' in site_dict:\n","        if 'well pad' in site_dict and 'tank_battery' in site_dict:\n","            if any(x > 0.6 for x in site_dict['tank battery']) and any(x > 0.8 for x in site_dict['well pad']):\n","              #a ton of false positivites with tbs and seps, need a good pad threshhold\n","              tag.append(\"Upstream Facility\")\n","        if 'well pad' in site_dict and 'separator' in site_dict:\n","            if any(x > 0.8 for x in site_dict['well pad']) and any(x > 0.5 for x in site_dict['separator']):\n","              tag.append(\"Upstream Facility\")\n","\n","    # Case 3: Well Pad\n","    if 'artificial lift' in site_dict and 'well pad' in site_dict:\n","        if any(x > 0.5 for x in site_dict['artificial lift']) and any(x > 0.6 for x in site_dict['well pad']): #If theres a pump jack and somthing like a pad, its def a ped\n","            tag.append(\"Well Pad with Artificial Lift\")\n","\n","    # Case 4: Fallback Well Pad or None\n","    elif len(tag) == 0:\n","        if 'well pad' in site_dict:\n","            if any(x > 0.85 for x in site_dict['well pad']):\n","                tag.append(\"General Pad (exact identity unknown)\")\n","            else:\n","                tag = None\n","\n","    #We want a BAD tag for the model to throw out this data low score single pads are almost always not valuable sites\n","\n","    if tank_val == 0 and sep_val == 0 and comp_val == 0 and lift_val == 0: #Everything else needs to be zero to be sure\n","        if 'well pad' in site_dict and len(pad_val) == 1:\n","            if all(x < 0.7 for x in site_dict['well pad']):\n","                print(\"Low Confidence Pad Dectected, likley not a site\")\n","                tag = \"BAD\"\n","\n","    if tag == []:\n","      print(\"Site cannot be catagorized\")\n","      tag = None\n","\n","    return tag\n","\n","\n","\n"]},{"cell_type":"markdown","source":["## Exporting\n","Below will be a list of scripts that take our JSON file and convert it into stuff usable for viewers"],"metadata":{"id":"QYaH87TWxMR5"}},{"cell_type":"code","source":["import csv\n","import re\n","import ast"],"metadata":{"id":"0B6TsWJWxXQQ","executionInfo":{"status":"ok","timestamp":1752507149360,"user_tz":300,"elapsed":6,"user":{"displayName":"Max Bruce","userId":"03723525121176591960"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["file_path = os.path.join(root, \"486 Eagleford 714 Test_2025-07-14_16-05-54.json\") #paste name of JSON file\n","\n","def strip_np_float64(text):\n","    # Replace np.float64(123.456) → 123.456\n","    return re.sub(r'np\\.float64\\(([^()]+)\\)', r'\\1', text)\n","\n","def load_custom_dict(filepath):\n","    with open(filepath, 'r') as f:\n","        raw = f.read().strip()\n","\n","    # Remove outer quotes if present\n","    if raw.startswith('\"') and raw.endswith('\"'):\n","        raw = raw[1:-1]\n","\n","    # Strip np.float64() calls\n","    cleaned = strip_np_float64(raw)\n","\n","    # Now parse it safely\n","    data = ast.literal_eval(cleaned)\n","    return data\n","\n","data_dict = load_custom_dict(file_path)"],"metadata":{"id":"e-kbr-VFxnXb","executionInfo":{"status":"ok","timestamp":1752509229988,"user_tz":300,"elapsed":11,"user":{"displayName":"Max Bruce","userId":"03723525121176591960"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["data = data_dict\n","\n","# Output file name\n","filename = \"EF 200 Test.csv\"\n","\n","# Collect all possible fieldnames\n","fieldnames = ['latitude', 'longitude']\n","# Add any other keys found in the inner dictionaries\n","for value in data.values():\n","    for key in value.keys():\n","        if key not in fieldnames:\n","            fieldnames.append(key)\n","\n","# Write CSV\n","with open(filename, mode='w', newline='') as f:\n","    writer = csv.DictWriter(f, fieldnames=fieldnames)\n","    writer.writeheader()\n","\n","    for (lat, lon), values in data.items():\n","        row = {'latitude': lat, 'longitude': lon}\n","        row.update(values)  # Add the rest of the values\n","        writer.writerow(row)\n","\n","print(f\"CSV written to {filename}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"znjJjJV1xVsI","executionInfo":{"status":"ok","timestamp":1752509231794,"user_tz":300,"elapsed":18,"user":{"displayName":"Max Bruce","userId":"03723525121176591960"}},"outputId":"54c5ff8e-fdd6-4a85-8bfb-00b874c55e8f"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["CSV written to EF 200 Test.csv\n"]}]},{"cell_type":"markdown","source":["KML File importing"],"metadata":{"id":"3VZEcuCt0vRb"}},{"cell_type":"code","source":["import xml.etree.ElementTree as ET\n","\n","def extract_kml_polygon_coords(filepath):\n","    tree = ET.parse(filepath)\n","    root = tree.getroot()\n","\n","    # KML uses namespaces; we must define them\n","    ns = {'kml': 'http://www.opengis.net/kml/2.2'}\n","\n","    # Find all <coordinates> elements inside polygons\n","    coords = []\n","    for coord_tag in root.findall('.//kml:Polygon//kml:coordinates', ns):\n","        raw = coord_tag.text.strip()\n","        # Each point is \"lon,lat[,alt]\" and separated by spaces\n","        for line in raw.split():\n","            lon_lat = line.strip().split(',')[:2]  # Ignore altitude if present\n","            lon, lat = map(float, lon_lat)\n","            coords.append((lon, lat))\n","\n","    return coords"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SypLZZfw0u6-","executionInfo":{"status":"ok","timestamp":1752245873369,"user_tz":300,"elapsed":24,"user":{"displayName":"Max Bruce","userId":"03723525121176591960"}},"outputId":"db6fe9ed-8d10-43ae-87bc-8bca7589850a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(-97.73178684214577, 28.87748549626617), (-97.69376103536057, 28.90759227464033), (-97.62914833723072, 28.92292711105228), (-97.59450960413132, 28.9542944115274), (-97.57600515435394, 28.98947835115226), (-97.60520023844285, 29.02693342291385), (-97.61325172752822, 29.04305344782935), (-97.67062634340834, 29.07900027031357), (-97.71550413691881, 29.09239011428892), (-97.75390291902202, 29.10588023767992), (-97.76703427084135, 29.11109496668885), (-97.79330279096142, 29.05081135280534), (-97.81359428342346, 29.00120846817292), (-97.80862861530086, 28.95770880940933), (-97.81091538258264, 28.9343922586583), (-97.85455264326222, 28.90495864645925), (-97.87672726199224, 28.88480386867914), (-97.86157514672466, 28.8419900070367), (-97.81144822639831, 28.82714216766382), (-97.7561844924435, 28.84621963108358), (-97.73062716801826, 28.86613273808085), (-97.73178684214577, 28.87748549626617)]\n","[(-97.81802361108114, 28.95122558076191), (-97.86448084880845, 28.87971119491164), (-97.80308959867, 28.83799651731765), (-97.57199506609997, 28.99879929927403), (-97.75968536548972, 29.10189906975585), (-97.81802361108114, 28.95122558076191)]\n"]}]},{"cell_type":"markdown","source":["## Next stage of deployment"],"metadata":{"id":"lMYuZUxHjcBo"}},{"cell_type":"code","source":["poly_T4 = [(29.175, -97.600), (29.175, -97.475), (29.051, -97.475), (29.051, -97.600)]\n","coord_master_test(poly_T4, False, \"486 Eagleford 714 Test 2\", \"tb_model_v3.pth\")"],"metadata":{"collapsed":true,"id":"8KEwXDs_8mZV"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["QYaH87TWxMR5"],"authorship_tag":"ABX9TyPPlVK1p1oN0ok8t5Xsag79"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
